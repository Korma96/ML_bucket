{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinetunningHuggingface.ipynb","provenance":[],"collapsed_sections":["8Vl9AlSGCMfS","WiKSihA2B_Dc"],"authorship_tag":"ABX9TyP5LpzzW3OAO3qrn5S68OU9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_-lepli1zgn4"},"source":["#Finetunning Distillbert"]},{"cell_type":"markdown","metadata":{"id":"8Vl9AlSGCMfS"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"Wctm4D3t725B"},"source":["Setup google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwG4HxMw7y5B","executionInfo":{"status":"ok","timestamp":1619010479191,"user_tz":-120,"elapsed":23724,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"f00118d1-12ac-49b4-9864-3cc5c07ffd93"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O3EJNMLrrbjJ"},"source":["Install libraries"]},{"cell_type":"code","metadata":{"id":"LAkN6mIcrWeC"},"source":["!pip install ray[tune]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YTkb_uPQ3HOz"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"sNESpxoZ3Kq9"},"source":["from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer, PreTrainedModel, \\\n","    AutoModelForSequenceClassification, TrainingArguments\n","from transformers import Trainer\n","import torch\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report,  precision_recall_fscore_support\n","import pandas as pd\n","from pandas import DataFrame\n","from enum import Enum\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2OXHLVe65aTC"},"source":["Constants"]},{"cell_type":"code","metadata":{"id":"pfZ1jnDp4qHp"},"source":["BASE_PATH = '/content/gdrive/My Drive/AI'\n","FORMATTED_DATA_PATH_TRAIN = BASE_PATH + '/notebooks/dataset/train-en.tsv'\n","FORMATTED_DATA_PATH_EVAL = BASE_PATH + '/notebooks/dataset/eval-en.tsv'\n","TRAINER_MODEL_SAVE_PATH = BASE_PATH + '/notebooks/output/models'\n","TRAINER_LOGS_SAVE_PATH = BASE_PATH + '/notebooks/output/logs'\n","MODEL_NAME = \"distilbert-base-uncased\"\n","RANDOM_STATE = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHSEnWNunUb-"},"source":["class Constants:\n","  TAB = '\\t'\n","  NEW_LINE = '\\n'\n","  REQUESTS = 'requests'\n","  INTENTS = 'intents'\n","\n","const = Constants"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkip8fIZ0Wtd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619011726253,"user_tz":-120,"elapsed":707,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"5f918ace-f67a-4f7f-f611-ea931fc1045e"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print('Number of devices:', torch.cuda.device_count())\n","print('Device name:', torch.cuda.get_device_name(0))\n","print('Device type:', device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of devices: 1\n","Device name: Tesla T4\n","Device type: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WiKSihA2B_Dc"},"source":["## Prepare data"]},{"cell_type":"code","metadata":{"id":"NAKHC7SDCzo7"},"source":["class DatasetType(Enum):\n","    TRAIN = 0,\n","    EVAL = 1,\n","    TEST = 2\n","\n","def read_data(dataset_type: DatasetType = DatasetType.TRAIN) -> DataFrame:\n","    if dataset_type == DatasetType.TRAIN:\n","        formatted_data_path = FORMATTED_DATA_PATH_TRAIN\n","    elif dataset_type == DatasetType.EVAL:\n","        formatted_data_path = FORMATTED_DATA_PATH_EVAL\n","    else:\n","        raise Exception('method parameter not valid')\n","\n","    return pd.read_csv(formatted_data_path, delimiter=const.TAB, names=[const.INTENTS, const.REQUESTS])\n","\n","\n","def maybe_replace(row: str, dictionary: dict) -> str:\n","    for key, value in dictionary.items():\n","        if key in row:\n","            row = row.replace(key, value)\n","    return row\n","\n","\n","def fix_misspelled_words(df: DataFrame) -> DataFrame:\n","    # obtained using SpellChecker package and spacy tokenization\n","    before_tokenization = {\"a.m.\": \"am\", \"a.m\": \"am\", \"p.m.\": \"pm\",\n","                           \"p.m\": \"pm\", \"wed.\": \"wednesday\", \"dr.\": \"doctor\"}\n","    after_tokenization = {\" f \": \" fahrenheit \", \" c \": \" celsius \", \" dr \": \" doctor \", \" appt \": \" appointment \", \" fl \": \" florida \",\n","                          \" st. \": \" saint \", \" st \": \" saint \", \" nyc \": \" new york city \", \" nc \": \" north carolina \",\n","                          \" nj \": \" new jersey \", \" dc \": \" district of columbia \", \"celcius\": \"celsius\", \" wed \": \" wednesday \",\n","                          \"mintues\": \"minutes\", \"snoozes\": \"snooze\", \"forcast\": \"forecast\", \"tempature\": \"temperature\",\n","                          \"tomorrow/\": \"tomorrow\", \"temperture\": \"temperature\", \" hrs \": \" hours \",\n","                          \"apointment\": \"appointment\", \"tempurature\": \"temperature\", \"bejing\": \" beijing \",\n","                          \" thurs \": \" thursday \", \" bday \": \" birthday \", \" avg \": \" average \", \"exerice\": \" exercise \",\n","                          \"altanta\": \" atlanta \"}\n","    for key, value in before_tokenization.items():                                                             # fix abbreviations\n","        df[const.REQUESTS] = df[const.REQUESTS].str.replace(key, value, regex=False)\n","    # unnecessary pycharm warning, should be ignored\n","    df[const.REQUESTS] = df.apply(lambda row: maybe_replace(row[const.REQUESTS], after_tokenization), axis=1)  # fix misspelled words\n","    return df\n","\n","\n","def preprocess(df: DataFrame, fix_misspelled=True, shuffle=True) -> DataFrame:\n","    df[const.REQUESTS] = list(map(lambda request: ' '.join(request.split()), df[const.REQUESTS]))  # fix whitespaces\n","    df[const.REQUESTS] = list(map(lambda request: request.lower(), df[const.REQUESTS]))            # lower\n","    to_remove = [\"weekdaily\"]                                                                      # remove strange words\n","    for word in to_remove:\n","        df = df[~df[const.REQUESTS].str.contains(word)]\n","    if fix_misspelled:\n","        fix_misspelled_words(df)\n","    df.drop_duplicates(subset=const.REQUESTS, inplace=True)                                        # drop duplicates\n","    if shuffle:\n","        df = df.sample(frac=1, random_state=RANDOM_STATE)                                                    # shuffle the DataFrame rows\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qVQ-j4eDD09"},"source":["class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        # encodings = {'input_ids': [...], 'attention_mask': [...] }\n","        item = {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}  # convert to torch tensor\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ald0VFwYBpX6"},"source":["def get_pt_dataset(dataset_type: DatasetType, tokenizer: PreTrainedTokenizer, num_of_samples: int = 1000) -> Dataset:\n","    \"\"\"\n","    TODO: use train_test_split instead of num_of_samples in order to keep label distribution\n","    \"\"\"\n","    df = read_data(dataset_type)\n","    df = preprocess(df, fix_misspelled=True, shuffle=True)\n","\n","    inputs = df[const.REQUESTS][:num_of_samples].tolist()\n","    encodings = tokenizer(inputs, truncation=True, padding=True)\n","\n","    labels = df[const.INTENTS][:num_of_samples].tolist()\n","    intent_to_id = get_intent_to_id()\n","    labels = [intent_to_id[label] for label in labels]\n","\n","    return CustomDataset(encodings, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pLmH7b1DQ-G"},"source":["def get_intent_to_id():\n","    return {\n","        'weather/find': 0,\n","        'alarm/set_alarm': 1,\n","        'alarm/show_alarms': 2,\n","        'reminder/set_reminder': 3,\n","        'alarm/modify_alarm': 4,\n","        'weather/checkSunrise': 5,\n","        'weather/checkSunset': 6,\n","        'alarm/snooze_alarm': 7,\n","        'alarm/cancel_alarm': 8,\n","        'reminder/show_reminders': 9,\n","        'reminder/cancel_reminder': 10,\n","        'alarm/time_left_on_alarm': 11\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fUO2kEJg2WFp"},"source":["## Deep Learning !"]},{"cell_type":"code","metadata":{"id":"jv8Go3Tn3UVZ"},"source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gLjAIY0pznmV"},"source":["def finetunning_trainer_class(model_name):\n","    # best run: learning_rate': 1.5930522616241033e-05, 'num_train_epochs': 3, 'seed': 28.614830534045772, 'per_device_train_batch_size': 4\n","    params = {\n","        'num_of_samples': 1000,  # -1 means all\n","        'train_batch_size': 4,\n","        'eval_batch_size': 4,\n","        'learning_rate': 1.5930522616241033e-05,\n","        'num_of_epochs': 3,\n","        'seed': 28.614830534045772\n","    }\n","\n","    # 1) prepare data\n","    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name)\n","    train_dataset = get_pt_dataset(DatasetType.TRAIN, tokenizer, params['num_of_samples'])\n","    eval_dataset = get_pt_dataset(DatasetType.EVAL, tokenizer, params['num_of_samples'])\n","\n","    # 2) prepare for training\n","    num_labels = len(get_intent_to_id().keys())\n","    # model with randomly initialized head\n","    model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","    model.to(device)\n","\n","    # 3) train\n","    training_args = TrainingArguments(\n","        output_dir=TRAINER_MODEL_SAVE_PATH,\n","        num_train_epochs=params['num_of_epochs'],\n","        per_device_train_batch_size=params['train_batch_size'],\n","        per_device_eval_batch_size=params['eval_batch_size'],\n","        #warmup_steps=5,\n","        #weight_decay=0.01,\n","        logging_dir=TRAINER_LOGS_SAVE_PATH\n","    )\n","\n","    # default optimizer is AdamW\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    setattr(trainer.args, 'seed', int(params['seed']))\n","\n","    print(trainer.train())\n","\n","    # 4) evaluate\n","    print(trainer.evaluate())\n","\n","    # 5) save\n","    trainer.save_model(TRAINER_MODEL_SAVE_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"djF-siUYqoIJ"},"source":["def hyperparameter_search(model_name):\n","    params = {\n","        'num_of_samples': 1000,\n","        'train_batch_size': 64,\n","        'eval_batch_size': 16,\n","        'learning_rate': 4.622589001020833e-05,\n","        'num_of_epochs': 3\n","    }\n","\n","    # 1) prepare data\n","    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name)\n","    train_dataset = get_pt_dataset(DatasetType.TRAIN, tokenizer, params['num_of_samples'])\n","    eval_dataset = get_pt_dataset(DatasetType.EVAL, tokenizer, params['num_of_samples'])\n","\n","    # 2) prepare for training\n","    num_labels = len(get_intent_to_id().keys())\n","\n","    # required for hyperparameter search\n","    def model_init():\n","        # model with randomly initialized head\n","        model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name,\n","                                                                                    num_labels=num_labels)\n","        return model\n","\n","    # 3) train\n","    training_args = TrainingArguments(\n","        output_dir=TRAINER_MODEL_SAVE_PATH,\n","        num_train_epochs=params['num_of_epochs'],\n","        per_device_train_batch_size=params['train_batch_size'],\n","        per_device_eval_batch_size=params['eval_batch_size'],\n","        #warmup_steps=5,\n","        #weight_decay=0.01,\n","        logging_dir=TRAINER_LOGS_SAVE_PATH\n","    )\n","\n","    # default optimizer is AdamW\n","    trainer = Trainer(\n","        model_init=model_init,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","    print('############ running hyperparameter_search')\n","    best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n","    # print(trainer.train())\n","    print(best_run)\n","    print()\n","    for n, v in best_run.hyperparameters.items():\n","        if n == 'seed':\n","          v = int(v)\n","        setattr(trainer.args, n, v)\n","    print('############ training model')\n","    trainer.train()\n","    print('############ evaluating model')\n","    # 4) evaluate\n","    print(trainer.evaluate())\n","\n","    # 5) save\n","    trainer.save_model(TRAINER_MODEL_SAVE_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oV0O1Slfmv5n","executionInfo":{"status":"ok","timestamp":1619019499261,"user_tz":-120,"elapsed":3802434,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"aa5cc2f4-d07b-4deb-f097-e04e98594e67"},"source":["finetunning_trainer_class(MODEL_NAME)\n","#hyperparameter_search(MODEL_NAME)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='17223' max='17223' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [17223/17223 1:02:33, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.417400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.120100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.125800</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.127100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.079000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.122800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.110700</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.084700</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.070300</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.094600</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.076800</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.095400</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.050900</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.084500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.069500</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.057600</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.049500</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.053300</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.062000</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.057600</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.043800</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.083300</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.066400</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.027400</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.047100</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.047600</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.041300</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.046100</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.048200</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.041400</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.037200</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.048100</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>0.039200</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.048900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["TrainOutput(global_step=17223, training_loss=0.07795547537844348, metrics={'train_runtime': 3753.574, 'train_samples_per_second': 4.588, 'total_flos': 747337481200800.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -494145536, 'train_mem_gpu_alloc_delta': 807544320, 'train_mem_cpu_peaked_delta': 494223360, 'train_mem_gpu_peaked_delta': 96468992})\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='946' max='946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [946/946 00:36]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["{'eval_loss': 0.05392073094844818, 'eval_accuracy': 0.9888918275588469, 'eval_f1': 0.9888918275588469, 'eval_precision': 0.9888918275588469, 'eval_recall': 0.9888918275588469, 'eval_runtime': 36.5253, 'eval_samples_per_second': 103.517, 'epoch': 3.0, 'eval_mem_cpu_alloc_delta': 1617920, 'eval_mem_gpu_alloc_delta': -3072, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 6530048}\n"],"name":"stdout"}]}]}