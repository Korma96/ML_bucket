{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"torchtext-tutorial.ipynb","provenance":[],"collapsed_sections":["OEYwq9uZ1z7X"],"authorship_tag":"ABX9TyPh0e2kh5HUwEGXEhD3fgb8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BNoaeZKq2CGm"},"source":["# Intro"]},{"cell_type":"markdown","metadata":{"id":"x_GT6QaF1G8n"},"source":["This notebook covers:</br>\n","- **torchtext**</br>\n","    - Loading data with torchtext</br>\n","    - Using torchtext Fields, Vocab</br>\n","    - Building iterator from Fields</br>\n","- Using **pretrained** word embeddings (FastText)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WqiwbEWKmQvg"},"source":["Loading custom dataset with torchtext [tutorial](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/A%20-%20Using%20TorchText%20with%20Your%20Own%20Datasets.ipynb#scrollTo=dSWFMam5VZeu)</br>\n","Torchtext in action [notebook](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb#scrollTo=-1_qUNzqlYK6)</br>\n","Torchtext multiclass classification [tutorial](https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/5%20-%20Multi-class%20Sentiment%20Analysis.ipynb#scrollTo=jR7tZ_5mxDAH)"]},{"cell_type":"markdown","metadata":{"id":"Ctt6CZf336Vy"},"source":["**Note**: Textual explanations are took from aforementioned notebooks"]},{"cell_type":"markdown","metadata":{"id":"_iCJSpz__khj"},"source":["TODO:</br>\n","Try to use torchtext pipeline for preprocessing"]},{"cell_type":"markdown","metadata":{"id":"Peug-yeB1_gU"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"7r5kpb8K4GDA","executionInfo":{"status":"ok","timestamp":1631780042769,"user_tz":-120,"elapsed":7182,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["!python -m spacy download en_core_web_sm &> /dev/null"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"YP0S9rTN0_lm","executionInfo":{"status":"ok","timestamp":1631783093377,"user_tz":-120,"elapsed":234,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from torchtext.legacy import data\n","from torchtext.vocab import FastText\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import pandas as pd\n","import spacy\n","import numpy as np\n","from sklearn.metrics import f1_score\n","\n","from google_drive_downloader import GoogleDriveDownloader as gdd\n","from pathlib import Path\n","from tqdm import tqdm\n","import json\n","import time"],"execution_count":342,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcZsTLPlPi1D","executionInfo":{"status":"ok","timestamp":1631780046953,"user_tz":-120,"elapsed":24,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["TRAIN_DATASET_PATH = '/tmp/train-en.tsv'\n","TRAIN_JSON_DATASET_PATH = '/tmp/train-en.json'\n","EVAL_DATASET_PATH = '/tmp/eval-en.tsv'\n","EVAL_JSON_DATASET_PATH = '/tmp/eval-en.json'\n","TRAIN_DATASET_GDRIVE_ID = '196d23FA_YFJTpu_yDRpXjBG6DPKs9WI2'\n","EVAL_DATASET_GDRIVE_ID = '1p4O7Y2ePV17gPbwPTOio5EM7QMXHFI5V'\n","TEXT = 'text'\n","LABEL = 'label'\n","\n","MAX_VOCAB_SIZE = 25_000\n","BATCH_SIZE = 64"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTS0HpYLmMVs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631780050597,"user_tz":-120,"elapsed":3666,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"d42eec98-2276-4b4c-ce55-0463e37ae04b"},"source":["def download_data_from_gdrive(local_path, gdrive_file_id):\n","    if not Path(local_path).is_file():\n","        gdd.download_file_from_google_drive(\n","            file_id=gdrive_file_id,\n","            dest_path=local_path,\n","        )\n","download_data_from_gdrive(TRAIN_DATASET_PATH, TRAIN_DATASET_GDRIVE_ID)\n","download_data_from_gdrive(EVAL_DATASET_PATH, EVAL_DATASET_GDRIVE_ID)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 196d23FA_YFJTpu_yDRpXjBG6DPKs9WI2 into /tmp/train-en.tsv... Done.\n","Downloading 1p4O7Y2ePV17gPbwPTOio5EM7QMXHFI5V into /tmp/eval-en.tsv... Done.\n"]}]},{"cell_type":"code","metadata":{"id":"MRd3NfVe08np","executionInfo":{"status":"ok","timestamp":1631780050598,"user_tz":-120,"elapsed":10,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11DyHNrcN2J-"},"source":["# Prepare"]},{"cell_type":"markdown","metadata":{"id":"OEYwq9uZ1z7X"},"source":["## Why use JSON over CSV/TSV with torchtext?\n","\n","1. Your `csv` or `tsv` data cannot store lists. This means data cannot be already tokenized, thus everytime you run your Python script that reads this data via TorchText, it has to be tokenized. Using advanced tokenizers, such as the `spaCy` tokenizer, takes a non-negligible amount of time. Thus, it is better to tokenize your datasets and store them in the `json lines` format.\n","\n","2. If tabs appear in your `tsv` data, or commas appear in your `csv` data, TorchText will think they are delimiters between columns. This will cause your data to be parsed incorrectly. Worst of all TorchText will not alert you to this as it cannot tell the difference between a tab/comma in a field and a tab/comma as a delimiter. As `json` data is essentially a dictionary, you access the data within the fields via its key, so do not have to worry about \"surprise\" delimiters.\n","</br>\n","\n","**JSON example**\n","```\n","{\"name\": \"John\", \"location\": \"United Kingdom\", \"age\": 42, \"quote\": [\"i\", \"love\", \"the\", \"united kingdom\"]}\n","{\"name\": \"Mary\", \"location\": \"United States\", \"age\": 36, \"quote\": [\"i\", \"want\", \"more\", \"telescopes\"]}\n","```\n","\n","**TSV example**\n","```\n","name\tlocation\tage\tquote\n","John\tUnited Kingdom\t42\ti love the united kingdom\n","Mary\tUnited States\t36\ti want more telescopes\n","```"]},{"cell_type":"markdown","metadata":{"id":"KhQe9Ya2bVeR"},"source":["Preprocess data"]},{"cell_type":"code","metadata":{"id":"abnE3mowQNRt","executionInfo":{"status":"ok","timestamp":1631780051522,"user_tz":-120,"elapsed":932,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n","\n","def clean(text):\n","    text = text.lower()\n","    text = text.strip() \n","    text = ' '.join(text.split())  # replace whitespace with single space\n","\n","    doc = nlp(text)\n","    tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    tokens = [token for token in tokens if len(token) > 1]\n","\n","    return ' '.join(tokens)\n","\n","def preprocess(df):\n","    df.drop_duplicates(subset=[TEXT], inplace=True)\n","    texts = df[TEXT].tolist()\n","    texts_clean = [clean(x) for x in tqdm(texts)]\n","    df[TEXT] = texts_clean\n","    df.drop_duplicates(subset=[TEXT], inplace=True)\n","    df[TEXT] = df[TEXT].apply(lambda x: x.split())\n","    return df[TEXT].tolist(), df[LABEL].tolist()\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1pRxr1DqQZ5","executionInfo":{"status":"ok","timestamp":1631780136016,"user_tz":-120,"elapsed":84501,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"134c9458-2c82-4906-b4d8-e93e24ad241e"},"source":["def prepare_json_file(input_data_path, output_data_path):\n","    df = pd.read_csv(input_data_path, delimiter='\\t', names=[LABEL, TEXT])\n","    texts, labels = preprocess(df)\n","\n","    temp_list = []\n","    for t, l in zip(texts, labels):\n","        temp_list.append(json.dumps({LABEL: l, TEXT: t}))\n","\n","    with open(output_data_path, 'w') as f:\n","        f.write('\\n'.join(temp_list))\n","\n","prepare_json_file(TRAIN_DATASET_PATH, TRAIN_JSON_DATASET_PATH)\n","prepare_json_file(EVAL_DATASET_PATH, EVAL_JSON_DATASET_PATH)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 24551/24551 [01:12<00:00, 336.93it/s]\n","100%|██████████| 3894/3894 [00:11<00:00, 341.23it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"aBj7Y-pu2xHD"},"source":["# Torchtext Dataset Iterator"]},{"cell_type":"markdown","metadata":{"id":"hS68T6Ko4NYx"},"source":["\n","Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces. We also need to specify a `tokenizer_language` which tells torchtext which spaCy model to use. We use the `en_core_web_sm` model which has to be downloaded with `python -m spacy download en_core_web_sm` before you run this notebook!\n","\n","`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels.\n","\n","More on [Field](https://pytorch.org/text/0.8.1/data.html#field)\n","\n","We also set the random seeds for reproducibility."]},{"cell_type":"code","metadata":{"id":"wEWa5Gx13LTl","executionInfo":{"status":"ok","timestamp":1631783362760,"user_tz":-120,"elapsed":411,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["SEED = 42\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True  # also used for reproducibility\n","\n","TEXT_FIELD = data.Field(tokenize = 'spacy',\n","                        tokenizer_language = 'en_core_web_sm')\n","LABEL_FIELD = data.LabelField()"],"execution_count":433,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WbKr2p_lV5x"},"source":["Next, we must tell TorchText which fields apply to which elements of the `json` object. \n","\n","For `json` data, we must create a dictionary where:\n","- the key matches the key of the `json` object\n","- the value is a tuple where:\n","  - the first element becomes the batch object's attribute name\n","  - the second element is the name of the `Field`\n","  \n","What do we mean when we say \"becomes the batch object's attribute name\"? Recall in the previous exercises where we accessed the `TEXT` and `LABEL` fields in the train/evaluation loop by using `batch.text` and `batch.label`, this is because TorchText sets the batch object to have a `text` and `label` attribute, each being a tensor containing either the text or the label.\n","\n","A few notes:\n","\n","* The order of the keys in the `fields` dictionary does not matter, as long as its keys match the `json` data keys.\n","\n","- When dealing with `json` data, not all of the keys have to be used\n","\n","- Also, if the values of `json` field are a string then the `Fields` tokenization is applied (default is to split the string on spaces), however if the values are a list then no tokenization is applied. Usually it is a good idea for the data to already be tokenized into a list, this saves time as you don't have to wait for TorchText to do it.\n","\n","- The value of the `json` fields do not have to be the same type. Some examples can have their `\"text\"` as a string, and some as a list. The tokenization will only get applied to the ones with their `\"text\"` as a string.\n","\n","- If you are using a `json` field, every single example must have an instance of that field, e.g. in this example all examples must have a text and label. However, if you are not using some field, it does not matter if an example does not have it."]},{"cell_type":"code","metadata":{"id":"U-bsTZio5DLC","executionInfo":{"status":"ok","timestamp":1631783362761,"user_tz":-120,"elapsed":3,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["fields = {TEXT: ('t', TEXT_FIELD), LABEL: ('l', LABEL_FIELD)} "],"execution_count":434,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6zCBl0xm-ld"},"source":["Now, in a training loop we can iterate over the data iterator and access the text via `batch.t` and the label via `batch.l`.\n","\n","We then create our datasets (`train_data` and `eval_data`) with the `TabularDataset.splits` function. \n","\n","The `path` argument specifices the top level folder common among both datasets, and the `train` and `eval` arguments specify the filename of each dataset, e.g. here the train dataset is located at `tmp/train-en.json`.\n","\n","We tell the function we are using `json` data, and pass in our `fields` dictionary defined previously."]},{"cell_type":"code","metadata":{"id":"C6jqqp88nUG3","executionInfo":{"status":"ok","timestamp":1631783363111,"user_tz":-120,"elapsed":352,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["train_data, eval_data = data.TabularDataset.splits(\n","                            path = '/tmp',\n","                            train = 'train-en.json',\n","                            validation = 'eval-en.json',\n","                            format = 'json',\n","                            fields = fields\n",")"],"execution_count":435,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTfRsCdary5M","executionInfo":{"status":"ok","timestamp":1631783363111,"user_tz":-120,"elapsed":4,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"c903aa07-8214-4686-bc99-3ed02f27d283"},"source":["print(train_data[0])\n","print(vars(train_data[0]))"],"execution_count":436,"outputs":[{"output_type":"stream","name":"stdout","text":["<torchtext.legacy.data.example.Example object at 0x7f9b79b5afd0>\n","{'t': ['tell', 'weather', 'report', 'half', 'moon', 'bay'], 'l': 'weather/find'}\n"]}]},{"cell_type":"code","metadata":{"id":"9LpRpfTory-_","executionInfo":{"status":"ok","timestamp":1631783363557,"user_tz":-120,"elapsed":449,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["TEXT_FIELD.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, vectors='fasttext.simple.300d', unk_init = torch.Tensor.normal_)\n","LABEL_FIELD.build_vocab(train_data)"],"execution_count":437,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEcJ5ak6tEZ3","executionInfo":{"status":"ok","timestamp":1631783363558,"user_tz":-120,"elapsed":21,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"cd3dd8ff-b21d-4f1c-b71c-8ea8fac1eecf"},"source":["vars(LABEL_FIELD.vocab)"],"execution_count":438,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'freqs': Counter({'alarm/cancel_alarm': 624,\n","          'alarm/modify_alarm': 291,\n","          'alarm/set_alarm': 2003,\n","          'alarm/show_alarms': 237,\n","          'alarm/snooze_alarm': 110,\n","          'alarm/time_left_on_alarm': 97,\n","          'reminder/cancel_reminder': 593,\n","          'reminder/set_reminder': 3688,\n","          'reminder/show_reminders': 238,\n","          'weather/checkSunrise': 48,\n","          'weather/checkSunset': 63,\n","          'weather/find': 7550}),\n"," 'itos': ['weather/find',\n","  'reminder/set_reminder',\n","  'alarm/set_alarm',\n","  'alarm/cancel_alarm',\n","  'reminder/cancel_reminder',\n","  'alarm/modify_alarm',\n","  'reminder/show_reminders',\n","  'alarm/show_alarms',\n","  'alarm/snooze_alarm',\n","  'alarm/time_left_on_alarm',\n","  'weather/checkSunset',\n","  'weather/checkSunrise'],\n"," 'stoi': defaultdict(None,\n","             {'alarm/cancel_alarm': 3,\n","              'alarm/modify_alarm': 5,\n","              'alarm/set_alarm': 2,\n","              'alarm/show_alarms': 7,\n","              'alarm/snooze_alarm': 8,\n","              'alarm/time_left_on_alarm': 9,\n","              'reminder/cancel_reminder': 4,\n","              'reminder/set_reminder': 1,\n","              'reminder/show_reminders': 6,\n","              'weather/checkSunrise': 11,\n","              'weather/checkSunset': 10,\n","              'weather/find': 0}),\n"," 'unk_index': None,\n"," 'vectors': None}"]},"metadata":{},"execution_count":438}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ky5T6DdGuob6","executionInfo":{"status":"ok","timestamp":1631783363558,"user_tz":-120,"elapsed":16,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"f0899c59-ec5c-4a09-dfb8-e86fb7dc3ba5"},"source":["TEXT_FIELD.vocab.itos[:10]"],"execution_count":439,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<unk>',\n"," '<pad>',\n"," 'alarm',\n"," 'remind',\n"," 'set',\n"," 'today',\n"," 'tomorrow',\n"," 'reminder',\n"," 'weather',\n"," 'go']"]},"metadata":{},"execution_count":439}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGMFRkACG_X4","executionInfo":{"status":"ok","timestamp":1631783363559,"user_tz":-120,"elapsed":14,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"81ece836-9c83-4b80-b333-ec9678d1baad"},"source":["print(TEXT_FIELD.vocab.vectors.shape)"],"execution_count":440,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3448, 300])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WNbxh3b9OgR","executionInfo":{"status":"ok","timestamp":1631783363559,"user_tz":-120,"elapsed":12,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"822a80d0-c394-49b5-9308-00a115a47f3d"},"source":["index = 1\n","print(len(TEXT_FIELD.vocab.vectors[index]))\n","print(TEXT_FIELD.vocab.itos[index])\n","print(TEXT_FIELD.vocab.vectors[index][:10])"],"execution_count":441,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n","<pad>\n","tensor([ 0.7694,  2.5574,  0.5716,  1.3596,  0.4334, -0.7172,  1.0554, -1.4534,\n","         1.7361,  1.8350])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h2XKdOOG-JZl","executionInfo":{"status":"ok","timestamp":1631783363560,"user_tz":-120,"elapsed":11,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"559d04dc-5158-41a0-b021-fe06206ad125"},"source":["word = 'alarms'\n","print(TEXT_FIELD.vocab.stoi[word])\n","print(TEXT_FIELD.vocab.vectors[TEXT_FIELD.vocab.stoi[word]][:10])"],"execution_count":442,"outputs":[{"output_type":"stream","name":"stdout","text":["2054\n","tensor([ 0.0767, -0.0613, -0.1499,  0.3306,  0.3727, -0.0158, -0.2204,  0.2401,\n","         0.4039, -0.1509])\n"]}]},{"cell_type":"markdown","metadata":{"id":"yNOE1Y1rtWjF"},"source":["Then, we can create the iterators after defining our batch size and device.\n","\n","By default, the train data is shuffled each epoch, but the validation/test data is sorted. However, TorchText doesn't know what to use to sort our data and it would throw an error if we don't tell it. \n","\n","There are two ways to handle this, you can either tell the iterator not to sort the validation/test data by passing `sort = False`, or you can tell it how to sort the data by passing a `sort_key`. A sort key is a function that returns a key on which to sort the data on. For example, `lambda x: x.s` will sort the examples by their `s` attribute, i.e their quote. Ideally, you want to use a sort key as the `BucketIterator` will then be able to sort your examples and then minimize the amount of padding within each batch.\n","\n","We can then iterate over our iterator to get batches of data. Note how by default TorchText has the batch dimension second."]},{"cell_type":"code","metadata":{"id":"yaaK99jouxv4","executionInfo":{"status":"ok","timestamp":1631783363806,"user_tz":-120,"elapsed":256,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["train_iterator, eval_iterator = data.BucketIterator.splits(\n","    (train_data, eval_data),\n","    sort_key = lambda x: x.t,  # sort by t attribute (text)\n","    batch_size=BATCH_SIZE,\n","    device=device)"],"execution_count":443,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOrrBxjLzgB8"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"6G_5I22zbDaW","executionInfo":{"status":"ok","timestamp":1631784969129,"user_tz":-120,"elapsed":444,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["fastText = FastText('simple')\n","oovs = []\n","vocab_shape = TEXT_FIELD.vocab.vectors.shape\n","weights_matrix = np.zeros((vocab_shape[0], vocab_shape[1]))\n","\n","for i, word in enumerate(TEXT_FIELD.vocab.itos):\n","    try: \n","        weights_matrix[i] = fastText[word]\n","    except KeyError:\n","        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n","        oovs.append(word)"],"execution_count":545,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BF1MK3Eub8Rg","executionInfo":{"status":"ok","timestamp":1631784969130,"user_tz":-120,"elapsed":5,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"1a306e36-7e6f-4676-e299-0dc794a7ba1f"},"source":["print(len(oovs))"],"execution_count":546,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","metadata":{"id":"tKdiTanXcWys","executionInfo":{"status":"ok","timestamp":1631784969130,"user_tz":-120,"elapsed":5,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["def create_emb_layer(weights_matrix, trainable=True):\n","    num_embeddings, embedding_dim = weights_matrix.shape\n","    emb_layer = nn.EmbeddingBag(num_embeddings, embedding_dim)  # averages word embeddings\n","    emb_layer.weight=nn.Parameter(torch.tensor(weights_matrix,dtype=torch.float32))\n","    if not trainable:\n","        emb_layer.weight.requires_grad = False  # default value is True\n","\n","    return emb_layer, num_embeddings, embedding_dim"],"execution_count":547,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-d-NA6jp6OJ-"},"source":["Dropout is implemented by initializing an `nn.Dropout` layer (the argument is the probability of dropping out each neuron) and using it within the `forward` method after each layer we want to apply dropout to. **Note**: never use dropout on the input or output layers, you only ever want to use dropout on hidden layers."]},{"cell_type":"markdown","metadata":{"id":"P9HOWPbjE7Dk"},"source":["Shoud you fine tune word embeddings? [link](https://stackoverflow.com/questions/58630101/using-torch-nn-embedding-for-glove-should-we-fine-tune-the-embeddings-or-just-u)"]},{"cell_type":"code","metadata":{"id":"vsGRbnFgxJSV","executionInfo":{"status":"ok","timestamp":1631784969130,"user_tz":-120,"elapsed":4,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["class Logistic_Regression(nn.Module):\n","    def __init__(self, weights_matrix, hidden_dim, output_dim, dropout):\n","        super(Logistic_Regression, self).__init__()\n","        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, False)\n","        self.hidden_layer = nn.Linear(embedding_dim, hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.output_layer = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x.view(x.shape[1], x.shape[0]))  # set batch size to be first dimension\n","        x = self.dropout(self.hidden_layer(x))\n","        x = self.output_layer(x)\n","        return x"],"execution_count":548,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPgJpHkn7Ba-","executionInfo":{"status":"ok","timestamp":1631784969380,"user_tz":-120,"elapsed":254,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["model = Logistic_Regression(weights_matrix=weights_matrix,  # dimension of FastText embeddings\n","                            hidden_dim=128,\n","                            output_dim=len(LABEL_FIELD.vocab),\n","                            dropout=0.1)  "],"execution_count":549,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NuwNmHBU_vmD","executionInfo":{"status":"ok","timestamp":1631784969381,"user_tz":-120,"elapsed":3,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"4258d5ee-3aff-4b68-809a-d9b3d8b83478"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":550,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 40,076 trainable parameters\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOZKVtYCBsUm","executionInfo":{"status":"ok","timestamp":1631784969381,"user_tz":-120,"elapsed":2,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"a8974983-7e63-4a7f-d53a-59da48a7bf43"},"source":["for name, param in model.named_parameters(): \n","    print(name, '\\ttrainable='+str(param.requires_grad))"],"execution_count":551,"outputs":[{"output_type":"stream","name":"stdout","text":["embedding.weight \ttrainable=False\n","hidden_layer.weight \ttrainable=True\n","hidden_layer.bias \ttrainable=True\n","output_layer.weight \ttrainable=True\n","output_layer.bias \ttrainable=True\n"]}]},{"cell_type":"markdown","metadata":{"id":"slFUUl2a7FC7"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"P8IRLV4nWY5A","executionInfo":{"status":"ok","timestamp":1631784970706,"user_tz":-120,"elapsed":3,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["softmax = nn.Softmax()\n","def calculate_f1(outputs, targets):\n","    out = softmax(outputs)\n","    y_pred = torch.argmax(out, dim=1).tolist()\n","    y = targets.tolist()\n","    return f1_score(y, y_pred, average='micro')"],"execution_count":552,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBqr1vDJYUxi"},"source":["**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."]},{"cell_type":"code","metadata":{"id":"KMr9x2dqTJCN","executionInfo":{"status":"ok","timestamp":1631784970706,"user_tz":-120,"elapsed":3,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_f1 = 0\n","\n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        predictions = model(batch.t)\n","        \n","        loss = criterion(predictions, batch.l)\n","        \n","        f1 = calculate_f1(predictions, batch.l)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_f1 += f1\n","        \n","    return epoch_loss / len(iterator), epoch_f1 / len(iterator)"],"execution_count":553,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmVfSB2oYaBZ"},"source":["**Note**: as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."]},{"cell_type":"code","metadata":{"id":"mB2vXbd6YNzL","executionInfo":{"status":"ok","timestamp":1631784970707,"user_tz":-120,"elapsed":4,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_f1 = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            \n","            predictions = model(batch.t)\n","            \n","            loss = criterion(predictions, batch.l)\n","            \n","            f1 = calculate_f1(predictions, batch.l)\n","\n","            epoch_loss += loss.item()\n","            epoch_f1 += f1\n","        \n","    return epoch_loss / len(iterator), epoch_f1 / len(iterator)"],"execution_count":554,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Wn_Xr30YrSW","executionInfo":{"status":"ok","timestamp":1631784970707,"user_tz":-120,"elapsed":3,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":555,"outputs":[]},{"cell_type":"code","metadata":{"id":"H35hUmiKFOvN","executionInfo":{"status":"ok","timestamp":1631784970945,"user_tz":-120,"elapsed":241,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}}},"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss()\n","#criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":556,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_hRfUJdPY1bB"},"source":["Let's train!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEskXF04Y0mU","executionInfo":{"status":"ok","timestamp":1631784983873,"user_tz":-120,"elapsed":4599,"user":{"displayName":"Marko Radovic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghx1XW2yAOndMY6VpQCortiRa_k4FwEoWv98wQ9=s64","userId":"05554837114221040480"}},"outputId":"f4f0c08b-82a5-4f9d-8471-baf2dc6597ad"},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_f1 = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_f1 = evaluate(model, eval_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train F1: {train_f1*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1: {valid_f1*100:.2f}%')"],"execution_count":558,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.526 | Train F1: 48.57%\n","\t Val. Loss: 1.541 |  Val. F1: 48.80%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.525 | Train F1: 48.57%\n","\t Val. Loss: 1.533 |  Val. F1: 48.80%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.525 | Train F1: 48.57%\n","\t Val. Loss: 1.555 |  Val. F1: 48.80%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.525 | Train F1: 48.57%\n","\t Val. Loss: 1.530 |  Val. F1: 48.80%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 1.525 | Train F1: 48.58%\n","\t Val. Loss: 1.536 |  Val. F1: 48.80%\n"]}]}]}